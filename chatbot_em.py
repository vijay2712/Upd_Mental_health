# -*- coding: utf-8 -*-
# """Chatbot.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1MJeiO_OcWcYagzk2Lxr9md8jhoiG5ywT
# """

# !pip install tensorflow-gpu==2.4.2  
# !pip install 'h5py==2.10.0' --force-reinstall

# !pip install numpy==1.19.5

# !pip install transformers

# !pip install ktrain

from __future__ import absolute_import, division, print_function, unicode_literals

import sys

# !pip install tensorflow==2.3.1
# !pip install tensorflow-gpu==2.4.2 
# !pip install numpy==1.19.5
# !pip install tensorflow-datasets==4.1.0
import tensorflow as tf

tf.random.set_seed(1234)
AUTO = tf.data.experimental.AUTOTUNE

import tensorflow_datasets as tfds
import pandas as pd
import os
import re
import numpy as np
from time import time
import matplotlib.pyplot as plt
import h5py
from transformers import BertTokenizer

import pandas as pd
import ktrain
from ktrain import text
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings("ignore")

print("Tensorflow version {}".format(tf.__version__))
print("Numpy version {}".format(np.__version__))
print(h5py.__version__)

try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('Running on TPU {}'.format(tpu.cluster_spec().as_dict()['worker']))
except ValueError:
    tpu = None

if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
    strategy = tf.distribute.get_strategy()

print("REPLICAS: {}".format(strategy.num_replicas_in_sync))

"""**Hyperparameter**"""

# Maximum sentence length
MAX_LENGTH = 40

# Maximum number of samples to preprocess
MAX_SAMPLES = 10000

# For tf.data.Dataset
BATCH_SIZE = 64 * strategy.num_replicas_in_sync
BUFFER_SIZE = 20000

# For Transformer
NUM_LAYERS = 2
D_MODEL = 256
NUM_HEADS = 8
UNITS = 512
DROPOUT = 0.1

EPOCHS = 40

# from google.colab import drive
# drive.mount('/content/drive')
#read the dataset from google drive

FD_Training_dataset="C:/Users/Admin/Desktop/Final_Mental_health_project/static/csv/training_dataset_new.csv"
CC_Training_dataset="C:/Users/Admin/Desktop/Final_Mental_health_project/static/csv/20200325_counsel_chat.csv"
Emotion_model ="C:/Users/Admin/Desktop/Final_Mental_health_project/models/Emotion_detection_model"

model_cc="C:/Users/Admin/Desktop/Final_Mental_health_project/models/model_cc_len125_epc500_ac68.h5"
model_FD="C:/Users/Admin/Desktop/Final_Mental_health_project/models/model_ED_QA_ep500_ac33.h5"

data1 = pd.read_csv(FD_Training_dataset, encoding = 'utf-8')
data1.head()

#Drop uncessary columns
data1 = data1.drop('conv_id', axis = 1)
data1 = data1.drop('utterance_idx', axis = 1)
data1 = data1.drop('speaker_idx', axis = 1)
data1 = data1.drop('selfeval', axis = 1)
data1 = data1.drop('tags', axis = 1)
data1 = data1.dropna()
data1.head()

context = data1['context']
question = data1['prompt']
answer = data1['utterance']

# print(len(question))
# print(len(answer))


# Maximum number of samples to preprocess
MAX_SAMPLES = 50000

def preprocess_sentence(sentence):
    sentence = sentence.lower().strip()
    sentence = re.sub(r"([?.!,])", r" \1 ", sentence)
    sentence = re.sub(r'[" "]+', " ", sentence)
    sentence = re.sub(r"[^a-zA-Z?.!,]+", " ", sentence)
    sentence = sentence.strip()

    return sentence


questions = [preprocess_sentence(sentence) for sentence in question]
answers = [preprocess_sentence(sentence) for sentence in answer]

"""**Tokenizer**"""

# Build tokenizer using tfds for both questions and answers
tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(
    questions + answers, target_vocab_size=2**13)

# Define start and end token to indicate the start and end of a sentence
START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]

# Vocabulary size plus start and end token
VOCAB_SIZE = tokenizer.vocab_size + 2

# Tokenize, filter and pad sentences
def tokenize_and_filter(inputs, outputs):
  tokenized_inputs, tokenized_outputs = [], []
  
  for (sentence1, sentence2) in zip(inputs, outputs):
    # tokenize sentence
    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN
    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN
    # check tokenized sentence max length
    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:
      tokenized_inputs.append(sentence1)
      tokenized_outputs.append(sentence2)
  
  # pad tokenized sentences
  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(
      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')
  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(
      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')
  
  return tokenized_inputs, tokenized_outputs


questions, answers = tokenize_and_filter(questions, answers)

# print('Vocab size: {}'.format(VOCAB_SIZE))
# print('Number of samples: {}'.format(len(questions)))
# print('Number of samples: {}'.format(len(answers)))

# decoder inputs use the previous target as input
# remove START_TOKEN from targets
# dataset = tf.data.Dataset.from_tensor_slices((
#     {
#         'inputs': questions,
#         'dec_inputs': answers[:, :-1]
#     },
#     {
#         'outputs': answers[:, 1:]
#     },
# ))

# dataset = dataset.cache()
# dataset = dataset.shuffle(BUFFER_SIZE)
# dataset = dataset.batch(BATCH_SIZE)
# dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)

# print(dataset)

"""**Attention**"""

def scaled_dot_product_attention(query, key, value, mask):
  """Calculate the attention weights. """
  matmul_qk = tf.matmul(query, key, transpose_b=True)

  # scale matmul_qk
  depth = tf.cast(tf.shape(key)[-1], tf.float32)
  logits = matmul_qk / tf.math.sqrt(depth)

  # add the mask to zero out padding tokens
  if mask is not None:
    logits += (mask * -1e9)

  # softmax is normalized on the last axis (seq_len_k)
  attention_weights = tf.nn.softmax(logits, axis=-1)

  output = tf.matmul(attention_weights, value)

  return output

"""**Multi-head attention**"""

class MultiHeadAttention(tf.keras.layers.Layer):

  def __init__(self, d_model, num_heads, name="multi_head_attention"):
    super(MultiHeadAttention, self).__init__(name=name)
    self.num_heads = num_heads
    self.d_model = d_model

    assert d_model % self.num_heads == 0

    self.depth = d_model // self.num_heads

    self.query_dense = tf.keras.layers.Dense(units=d_model)
    self.key_dense = tf.keras.layers.Dense(units=d_model)
    self.value_dense = tf.keras.layers.Dense(units=d_model)

    self.dense = tf.keras.layers.Dense(units=d_model)
  
  def get_config(self):
        config = super(MultiHeadAttention,self).get_config()
        config.update({
            'num_heads':self.num_heads,
            'd_model':self.d_model,
        })
        return config

  def split_heads(self, inputs, batch_size):
    inputs = tf.keras.layers.Lambda(lambda inputs:tf.reshape(
        inputs, shape=(batch_size, -1, self.num_heads, self.depth)))(inputs)
    return tf.keras.layers.Lambda(lambda inputs: tf.transpose(inputs, perm=[0, 2, 1, 3]))(inputs)

  def call(self, inputs):
    query, key, value, mask = inputs['query'], inputs['key'], inputs[
        'value'], inputs['mask']
    batch_size = tf.shape(query)[0]

    # linear layers
    query = self.query_dense(query)
    key = self.key_dense(key)
    value = self.value_dense(value)

    # split heads
    query = self.split_heads(query, batch_size)
    key = self.split_heads(key, batch_size)
    value = self.split_heads(value, batch_size)

    # scaled dot-product attention
    scaled_attention = scaled_dot_product_attention(query, key, value, mask)
    scaled_attention = tf.keras.layers.Lambda(lambda scaled_attention: tf.transpose(
        scaled_attention, perm=[0, 2, 1, 3]))(scaled_attention)

    # concatenation of heads
    concat_attention = tf.keras.layers.Lambda(lambda scaled_attention: tf.reshape(scaled_attention,
                                  (batch_size, -1, self.d_model)))(scaled_attention)

    # final linear layer
    outputs = self.dense(concat_attention)

    return outputs

"""**Masking**"""

def create_padding_mask(x):
  mask = tf.cast(tf.math.equal(x, 0), tf.float32)
  # (batch_size, 1, 1, sequence length)
  return mask[:, tf.newaxis, tf.newaxis, :]

"""**Look-ahead mask to mask the future tokens in a sequence. We also mask out pad tokens.**

i.e. To predict the third word, only the first and second word will be used
"""

def create_look_ahead_mask(x):
  seq_len = tf.shape(x)[1]
  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)
  padding_mask = create_padding_mask(x)
  return tf.maximum(look_ahead_mask, padding_mask)

"""**Positional encoding**"""

class PositionalEncoding(tf.keras.layers.Layer):

  def __init__(self, position, d_model):
    self.position = 8054
    self.d_model = 128
    super(PositionalEncoding, self).__init__()
    self.pos_encoding = self.positional_encoding(position, d_model)
  
  def get_config(self):

        config = super(PositionalEncoding, self).get_config()
        config.update({
            'position': self.position,
            'd_model': self.d_model,
            
        })
        return config

  def get_angles(self, position, i, d_model):
    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))
    return position * angles

  def positional_encoding(self, position, d_model):
    angle_rads = self.get_angles(
        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],
        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],
        d_model=d_model)
    # apply sin to even index in the array
    sines = tf.math.sin(angle_rads[:, 0::2])
    # apply cos to odd index in the array
    cosines = tf.math.cos(angle_rads[:, 1::2])

    pos_encoding = tf.concat([sines, cosines], axis=-1)
    pos_encoding = pos_encoding[tf.newaxis, ...]
    return tf.cast(pos_encoding, tf.float32)

  def call(self, inputs):
    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]

sample_pos_encoding = PositionalEncoding(50, 512)

# plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')
# plt.xlabel('Depth')
# plt.xlim((0, 512))
# plt.ylabel('Position')
# plt.colorbar()
# plt.show()

"""**Encoding layer**"""

def encoder_layer(units, d_model, num_heads, dropout, name="encoder_layer"):
  inputs = tf.keras.Input(shape=(None, d_model), name="inputs")
  padding_mask = tf.keras.Input(shape=(1, 1, None), name="padding_mask")

  attention = MultiHeadAttention(
      d_model, num_heads, name="attention")({
          'query': inputs,
          'key': inputs,
          'value': inputs,
          'mask': padding_mask
      })
  attention = tf.keras.layers.Dropout(rate=dropout)(attention)
  add_attention = tf.keras.layers.add([inputs,attention])
  attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)

  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)
  outputs = tf.keras.layers.Dense(units=d_model)(outputs)
  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
  add_attention = tf.keras.layers.add([attention,outputs])
  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name)

sample_encoder_layer = encoder_layer(
    units=512,
    d_model=128,
    num_heads=4,
    dropout=0.3,
    name="sample_encoder_layer")

def encoder(vocab_size,
            num_layers,
            units,
            d_model,
            num_heads,
            dropout,
            name="encoder"):
  inputs = tf.keras.Input(shape=(None,), name="inputs")
  padding_mask = tf.keras.Input(shape=(1, 1, None), name="padding_mask")

  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
  embeddings *= tf.keras.layers.Lambda(lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32)))(d_model)
  embeddings = PositionalEncoding(vocab_size,d_model)(embeddings)

  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)

  for i in range(num_layers):
    outputs = encoder_layer(
        units=units,
        d_model=d_model,
        num_heads=num_heads,
        dropout=dropout,
        name="encoder_layer_{}".format(i),
    )([outputs, padding_mask])

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name)

sample_encoder = encoder(
    vocab_size=8199,
    num_layers=2,
    units=512,
    d_model=128,
    num_heads=4,
    dropout=0.3,
    name="sample_encoder")

"""**Decoding layer**"""

def decoder_layer(units, d_model, num_heads, dropout, name="decoder_layer"):
  inputs = tf.keras.Input(shape=(None, d_model), name="inputs")
  enc_outputs = tf.keras.Input(shape=(None, d_model), name="encoder_outputs")
  look_ahead_mask = tf.keras.Input(
      shape=(1, None, None), name="look_ahead_mask")
  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')

  attention1 = MultiHeadAttention(
      d_model, num_heads, name="attention_1")(inputs={
          'query': inputs,
          'key': inputs,
          'value': inputs,
          'mask': look_ahead_mask
      })
  add_attention = tf.keras.layers.add([attention1,inputs])    
  attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)

  attention2 = MultiHeadAttention(
      d_model, num_heads, name="attention_2")(inputs={
          'query': attention1,
          'key': enc_outputs,
          'value': enc_outputs,
          'mask': padding_mask
      })
  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)
  add_attention = tf.keras.layers.add([attention2,attention1])
  attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)

  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)
  outputs = tf.keras.layers.Dense(units=d_model)(outputs)
  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
  add_attention = tf.keras.layers.add([outputs,attention2])
  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)

  return tf.keras.Model(
      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],
      outputs=outputs,
      name=name)

sample_decoder_layer = decoder_layer(
    units=512,
    d_model=128,
    num_heads=4,
    dropout=0.3,
    name="sample_decoder_layer")

def decoder(vocab_size,
            num_layers,
            units,
            d_model,
            num_heads,
            dropout,
            name='decoder'):
  inputs = tf.keras.Input(shape=(None,), name='inputs')
  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')
  look_ahead_mask = tf.keras.Input(
      shape=(1, None, None), name='look_ahead_mask')
  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')
  
  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
  embeddings *= tf.keras.layers.Lambda(lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32)))(d_model)
  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)

  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)

  for i in range(num_layers):
    outputs = decoder_layer(
        units=units,
        d_model=d_model,
        num_heads=num_heads,
        dropout=dropout,
        name='decoder_layer_{}'.format(i),
    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])

  return tf.keras.Model(
      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],
      outputs=outputs,
      name=name)

sample_decoder = decoder(
    vocab_size=8199,
    num_layers=2,
    units=512,
    d_model=128,
    num_heads=4,
    dropout=0.3,
    name="sample_decoder")

"""**Transformer**"""

def transformer(vocab_size,
                num_layers,
                units,
                d_model,
                num_heads,
                dropout,
                name="transformer"):
  inputs = tf.keras.Input(shape=(None,), name="inputs")
  dec_inputs = tf.keras.Input(shape=(None,), name="dec_inputs")

  enc_padding_mask = tf.keras.layers.Lambda(
      create_padding_mask, output_shape=(1, 1, None),
      name='enc_padding_mask')(inputs)
  # mask the future tokens for decoder inputs at the 1st attention block
  look_ahead_mask = tf.keras.layers.Lambda(
      create_look_ahead_mask,
      output_shape=(1, None, None),
      name='look_ahead_mask')(dec_inputs)
  # mask the encoder outputs for the 2nd attention block
  dec_padding_mask = tf.keras.layers.Lambda(
      create_padding_mask, output_shape=(1, 1, None),
      name='dec_padding_mask')(inputs)

  enc_outputs = encoder(
      vocab_size=vocab_size,
      num_layers=num_layers,
      units=units,
      d_model=d_model,
      num_heads=num_heads,
      dropout=dropout,
  )(inputs=[inputs, enc_padding_mask])

  dec_outputs = decoder(
      vocab_size=vocab_size,
      num_layers=num_layers,
      units=units,
      d_model=d_model,
      num_heads=num_heads,
      dropout=dropout,
  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])

  outputs = tf.keras.layers.Dense(units=vocab_size, name="outputs")(dec_outputs)

  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)

sample_transformer = transformer(
    vocab_size=8199,
    num_layers=4,
    units=512,
    d_model=128,
    num_heads=4,
    dropout=0.3,
    name="sample_transformer")

"""**Loss function**"""

def loss_function(y_true, y_pred):
  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))
  
  loss = tf.keras.losses.SparseCategoricalCrossentropy(
      from_logits=True, reduction='none')(y_true, y_pred)

  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)
  loss = tf.multiply(loss, mask)

  return tf.reduce_mean(loss)

"""**Custom learning rate**"""

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):

  def __init__(self, d_model, warmup_steps=4000):
    super(CustomSchedule, self).__init__()
    
    self.d_model = tf.constant(d_model,dtype=tf.float32)
    self.warmup_steps = warmup_steps
    
  def get_config(self):
        return {"d_model": self.d_model,"warmup_steps":self.warmup_steps}
    
  def __call__(self, step):
    arg1 = tf.math.rsqrt(step)
    arg2 = step * (self.warmup_steps**-1.5)

    return tf.math.multiply(tf.math.rsqrt(self.d_model), tf.math.minimum(arg1, arg2))

sample_learning_rate = CustomSchedule(d_model=128)

# plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))
# plt.ylabel("Learning Rate")
# plt.xlabel("Train Step")

"""**Initialize and compile model**"""

# clear backend
tf.keras.backend.clear_session()

learning_rate = CustomSchedule(D_MODEL)

optimizer = tf.keras.optimizers.Adam(
    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)

def accuracy(y_true, y_pred):
  # ensure labels have shape (batch_size, MAX_LENGTH - 1)
  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))
  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)

# initialize and compile model within strategy scope
with strategy.scope():
  loaded_model = transformer(
      vocab_size=VOCAB_SIZE,
      num_layers=NUM_LAYERS,
      units=UNITS,
      d_model=D_MODEL,
      num_heads=NUM_HEADS,
      dropout=DROPOUT)

  loaded_model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])

loaded_model.summary()

"""**Load FD Model**"""

loaded_model.load_weights(model_FD)
print("Loaded model from disk")

"""**Prediction**"""

def evaluate(sentence):
  sentence = preprocess_sentence(sentence)

  sentence = tf.expand_dims(
      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)

  output = tf.expand_dims(START_TOKEN, 0)

  for i in range(MAX_LENGTH):
    predictions = loaded_model(inputs=[sentence, output], training=False)

    # select the last word from the seq_len dimension
    predictions = predictions[:, -1:, :]
    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)

    # return the result if the predicted_id is equal to the end token
    if tf.equal(predicted_id, END_TOKEN[0]):
      break

    # concatenated the predicted_id to the output which is given to the decoder
    # as its input.
    output = tf.concat([output, predicted_id], axis=-1)

  return tf.squeeze(output, axis=0)


def predict(sentence):
  prediction = evaluate(sentence)

  predicted_sentence = tokenizer.decode(
      [i for i in prediction if i < tokenizer.vocab_size])

  # print('Input: {}'.format(sentence))
  # print("\n")
  # print('Output: {}'.format(predicted_sentence))

  return predicted_sentence

question = data1['prompt'][10]
answer = data1['utterance'][10]
emotion = data1['context'][10]
output = predict(question)
# print(question)
# print(answer)
# print(emotion)
# print(output)

"""**Load CC Model**"""

data = pd.read_csv(CC_Training_dataset, encoding = 'utf-8')
data.head()

#Drop uncessary columns
data=data.drop('Unnamed: 0', axis = 1)
data=data.drop('questionID', axis = 1)
data=data.drop('questionTitle', axis = 1)
data=data.drop('questionLink', axis = 1)
data=data.drop('therapistInfo', axis = 1)
data=data.drop('therapistURL', axis = 1)
data=data.drop('upvotes', axis = 1)
data=data.drop('views', axis = 1)
data.head()

#Split the dataset
data_train = data[data["split"] == "train"]
data_val = data[data["split"] == "val"]
data_testing = data[data["split"] == "test"]
# print(data_train.shape)
# print(data_val.shape)
# print(data_testing.shape)

context1 = data_train['topic']
question1 = data_train['questionText']
answer1 = data_train['answerText']

# print(len(question1))
# print(len(answer1))


# Maximum number of samples to preprocess
MAX_SAMPLES = 50000

def preprocess_sentence(sentence):
    sentence = sentence.lower().strip()
    sentence = re.sub(r"([?.!,])", r" \1 ", sentence)
    sentence = re.sub(r'[" "]+', " ", sentence)
    sentence = re.sub(r"[^a-zA-Z?.!,]+", " ", sentence)
    sentence = sentence.strip()

    return sentence


questions1 = [preprocess_sentence(sentence) for sentence in question1]
answers1 = [preprocess_sentence(sentence) for sentence in answer1]

# question1[10]

# Build tokenizer using tfds for both questions and answers
tokenizer1 = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(
    questions1 + answers1, target_vocab_size=2**13)

# Define start and end token to indicate the start and end of a sentence
START_TOKEN1, END_TOKEN1 = [tokenizer1.vocab_size], [tokenizer1.vocab_size + 1]

# Vocabulary size plus start and end token
VOCAB_SIZE1 = tokenizer1.vocab_size + 2

# print(VOCAB_SIZE1)
# print(START_TOKEN1)
# print(END_TOKEN1)

# print('Tokenized sample question: {}'.format(tokenizer1.encode(questions1[20])))

MAX_LENGTH1=125
# Tokenize, filter and pad sentences
def tokenize_and_filter1(inputs, outputs):
  tokenized_inputs, tokenized_outputs = [], []
  
  for (sentence1, sentence2) in zip(inputs, outputs):
    # tokenize sentence
    sentence1 = START_TOKEN1 + tokenizer1.encode(sentence1) + END_TOKEN1
    sentence2 = START_TOKEN1 + tokenizer1.encode(sentence2) + END_TOKEN1
    # check tokenized sentence max length
    if len(sentence1) <= MAX_LENGTH1 and len(sentence2) <= MAX_LENGTH1:
      tokenized_inputs.append(sentence1)
      tokenized_outputs.append(sentence2)
  
  # pad tokenized sentences
  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(
      tokenized_inputs, maxlen=MAX_LENGTH1, padding='post')
  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(
      tokenized_outputs, maxlen=MAX_LENGTH1, padding='post')
  
  return tokenized_inputs, tokenized_outputs


questions3, answers3 = tokenize_and_filter1(questions1, answers1)

# print(VOCAB_SIZE1)
# print(len(questions3))
# print(len(answers3))

# questions3[0]

def scaled_dot_product_attention1(query, key, value, mask):
  """Calculate the attention weights. """
  matmul_qk = tf.matmul(query, key, transpose_b=True)

  # scale matmul_qk
  depth = tf.cast(tf.shape(key)[-1], tf.float32)
  logits = matmul_qk / tf.math.sqrt(depth)

  # add the mask to zero out padding tokens
  if mask is not None:
    logits += (mask * -1e9)

  # softmax is normalized on the last axis (seq_len_k)
  attention_weights = tf.nn.softmax(logits, axis=-1)

  output = tf.matmul(attention_weights, value)

  return output

class MultiHeadAttention1(tf.keras.layers.Layer):

  def __init__(self, d_model, num_heads, name="multi_head_attention"):
    super(MultiHeadAttention1, self).__init__(name=name)
    self.num_heads = num_heads
    self.d_model = d_model

    assert d_model % self.num_heads == 0

    self.depth = d_model // self.num_heads

    self.query_dense = tf.keras.layers.Dense(units=d_model)
    self.key_dense = tf.keras.layers.Dense(units=d_model)
    self.value_dense = tf.keras.layers.Dense(units=d_model)

    self.dense = tf.keras.layers.Dense(units=d_model)
  
  def get_config(self):
        config = super(MultiHeadAttention1,self).get_config()
        config.update({
            'num_heads':self.num_heads,
            'd_model':self.d_model,
        })
        return config

  def split_heads(self, inputs, batch_size):
    inputs = tf.keras.layers.Lambda(lambda inputs:tf.reshape(
        inputs, shape=(batch_size, -1, self.num_heads, self.depth)))(inputs)
    return tf.keras.layers.Lambda(lambda inputs: tf.transpose(inputs, perm=[0, 2, 1, 3]))(inputs)

  def call(self, inputs):
    query, key, value, mask = inputs['query'], inputs['key'], inputs[
        'value'], inputs['mask']
    batch_size = tf.shape(query)[0]

    # linear layers
    query = self.query_dense(query)
    key = self.key_dense(key)
    value = self.value_dense(value)

    # split heads
    query = self.split_heads(query, batch_size)
    key = self.split_heads(key, batch_size)
    value = self.split_heads(value, batch_size)

    # scaled dot-product attention
    scaled_attention = scaled_dot_product_attention1(query, key, value, mask)
    scaled_attention = tf.keras.layers.Lambda(lambda scaled_attention: tf.transpose(
        scaled_attention, perm=[0, 2, 1, 3]))(scaled_attention)

    # concatenation of heads
    concat_attention = tf.keras.layers.Lambda(lambda scaled_attention: tf.reshape(scaled_attention,
                                  (batch_size, -1, self.d_model)))(scaled_attention)

    # final linear layer
    outputs = self.dense(concat_attention)

    return outputs

def create_padding_mask1(x):
  mask = tf.cast(tf.math.equal(x, 0), tf.float32)
  # (batch_size, 1, 1, sequence length)
  return mask[:, tf.newaxis, tf.newaxis, :]

# print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))

def create_look_ahead_mask1(x):
  seq_len = tf.shape(x)[1]
  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)
  padding_mask = create_padding_mask(x)
  return tf.maximum(look_ahead_mask, padding_mask)

# print(create_look_ahead_mask(tf.constant([[1, 2, 0, 4, 5]])))

class PositionalEncoding1(tf.keras.layers.Layer):

  def __init__(self, position, d_model):
    self.position = 8199
    self.d_model = 128
    super(PositionalEncoding1, self).__init__()
    self.pos_encoding1 = self.positional_encoding1(position, d_model)
  
  def get_config(self):

        config = super(PositionalEncoding1, self).get_config()
        config.update({
            'position': self.position,
            'd_model': self.d_model,
            
        })
        return config

  def get_angles(self, position, i, d_model):
    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))
    return position * angles

  def positional_encoding1(self, position, d_model):
    angle_rads = self.get_angles(
        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],
        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],
        d_model=d_model)
    # apply sin to even index in the array
    sines = tf.math.sin(angle_rads[:, 0::2])
    # apply cos to odd index in the array
    cosines = tf.math.cos(angle_rads[:, 1::2])

    pos_encoding1 = tf.concat([sines, cosines], axis=-1)
    pos_encoding1 = pos_encoding1[tf.newaxis, ...]
    return tf.cast(pos_encoding1, tf.float32)

  def call(self, inputs):
    return inputs + self.pos_encoding1[:, :tf.shape(inputs)[1], :]

sample_pos_encoding = PositionalEncoding1(50, 512)

# plt.pcolormesh(sample_pos_encoding.pos_encoding1.numpy()[0], cmap='RdBu')
# plt.xlabel('Depth')
# plt.xlim((0, 512))
# plt.ylabel('Position')
# plt.colorbar()
# plt.show()

def encoder_layer1(units, d_model, num_heads, dropout, name="encoder_layer"):
  inputs = tf.keras.Input(shape=(None, d_model), name="inputs")
  padding_mask = tf.keras.Input(shape=(1, 1, None), name="padding_mask")

  attention = MultiHeadAttention1(
      d_model, num_heads, name="attention")({
          'query': inputs,
          'key': inputs,
          'value': inputs,
          'mask': padding_mask
      })
  attention = tf.keras.layers.Dropout(rate=dropout)(attention)
  add_attention = tf.keras.layers.add([inputs,attention])
  attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)

  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)
  outputs = tf.keras.layers.Dense(units=d_model)(outputs)
  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
  add_attention = tf.keras.layers.add([attention,outputs])
  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name)

sample_encoder_layer1 = encoder_layer1(
    units=512,
    d_model=128,
    num_heads=4,
    dropout=0.3,
    name="sample_encoder_layer")

def encoder1(vocab_size,
            num_layers,
            units,
            d_model,
            num_heads,
            dropout,
            name="encoder"):
  inputs = tf.keras.Input(shape=(None,), name="inputs")
  padding_mask = tf.keras.Input(shape=(1, 1, None), name="padding_mask")

  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
  embeddings *= tf.keras.layers.Lambda(lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32)))(d_model)
  embeddings = PositionalEncoding1(vocab_size,d_model)(embeddings)

  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)

  for i in range(num_layers):
    outputs = encoder_layer1(
        units=units,
        d_model=d_model,
        num_heads=num_heads,
        dropout=dropout,
        name="encoder_layer_{}".format(i),
    )([outputs, padding_mask])

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name)

sample_encoder1 = encoder1(
    vocab_size=8199,
    num_layers=2,
    units=512,
    d_model=128,
    num_heads=4,
    dropout=0.3,
    name="sample_encoder")

def decoder_layer1(units, d_model, num_heads, dropout, name="decoder_layer"):
  inputs = tf.keras.Input(shape=(None, d_model), name="inputs")
  enc_outputs = tf.keras.Input(shape=(None, d_model), name="encoder_outputs")
  look_ahead_mask = tf.keras.Input(
      shape=(1, None, None), name="look_ahead_mask")
  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')

  attention1 = MultiHeadAttention1(
      d_model, num_heads, name="attention_1")(inputs={
          'query': inputs,
          'key': inputs,
          'value': inputs,
          'mask': look_ahead_mask
      })
  add_attention = tf.keras.layers.add([attention1,inputs])    
  attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)

  attention2 = MultiHeadAttention1(
      d_model, num_heads, name="attention_2")(inputs={
          'query': attention1,
          'key': enc_outputs,
          'value': enc_outputs,
          'mask': padding_mask
      })
  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)
  add_attention = tf.keras.layers.add([attention2,attention1])
  attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)

  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)
  outputs = tf.keras.layers.Dense(units=d_model)(outputs)
  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
  add_attention = tf.keras.layers.add([outputs,attention2])
  outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)

  return tf.keras.Model(
      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],
      outputs=outputs,
      name=name)

sample_decoder_layer1 = decoder_layer1(
    units=512,
    d_model=128,
    num_heads=4,
    dropout=0.3,
    name="sample_decoder_layer")

def decoder1(vocab_size,
            num_layers,
            units,
            d_model,
            num_heads,
            dropout,
            name='decoder'):
  inputs = tf.keras.Input(shape=(None,), name='inputs')
  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')
  look_ahead_mask = tf.keras.Input(
      shape=(1, None, None), name='look_ahead_mask')
  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')
  
  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
  embeddings *= tf.keras.layers.Lambda(lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32)))(d_model)
  embeddings = PositionalEncoding1(vocab_size, d_model)(embeddings)

  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)

  for i in range(num_layers):
    outputs = decoder_layer1(
        units=units,
        d_model=d_model,
        num_heads=num_heads,
        dropout=dropout,
        name='decoder_layer_{}'.format(i),
    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])

  return tf.keras.Model(
      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],
      outputs=outputs,
      name=name)

sample_decoder = decoder1(
    vocab_size=8199,
    num_layers=2,
    units=512,
    d_model=128,
    num_heads=4,
    dropout=0.3,
    name="sample_decoder")

def transformer1(vocab_size,
                num_layers,
                units,
                d_model,
                num_heads,
                dropout,
                name="transformer"):
  inputs = tf.keras.Input(shape=(None,), name="inputs")
  dec_inputs = tf.keras.Input(shape=(None,), name="dec_inputs")

  enc_padding_mask = tf.keras.layers.Lambda(
      create_padding_mask, output_shape=(1, 1, None),
      name='enc_padding_mask')(inputs)
  # mask the future tokens for decoder inputs at the 1st attention block
  look_ahead_mask = tf.keras.layers.Lambda(
      create_look_ahead_mask,
      output_shape=(1, None, None),
      name='look_ahead_mask')(dec_inputs)
  # mask the encoder outputs for the 2nd attention block
  dec_padding_mask = tf.keras.layers.Lambda(
      create_padding_mask, output_shape=(1, 1, None),
      name='dec_padding_mask')(inputs)

  enc_outputs = encoder1(
      vocab_size=vocab_size,
      num_layers=num_layers,
      units=units,
      d_model=d_model,
      num_heads=num_heads,
      dropout=dropout,
  )(inputs=[inputs, enc_padding_mask])

  dec_outputs = decoder1(
      vocab_size=vocab_size,
      num_layers=num_layers,
      units=units,
      d_model=d_model,
      num_heads=num_heads,
      dropout=dropout,
  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])

  outputs = tf.keras.layers.Dense(units=vocab_size, name="outputs")(dec_outputs)

  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)

# Maximum sentence length
MAX_LENGTH = 125

# Maximum number of samples to preprocess
MAX_SAMPLES = 50000

# For tf.data.Dataset
BATCH_SIZE = 64 * strategy.num_replicas_in_sync
BUFFER_SIZE = 20000

# For Transformer
NUM_LAYERS = 2
D_MODEL = 256
NUM_HEADS = 4
UNITS = 512
DROPOUT = 0.3

sample_transformer1 = transformer1(
    vocab_size=8199,
    num_layers=4,
    units=512,
    d_model=128,
    num_heads=4,
    dropout=0.3,
    name="sample_transformer")

def loss_function1(y_true, y_pred):
  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH1 - 1))
  
  loss = tf.keras.losses.SparseCategoricalCrossentropy(
      from_logits=True, reduction='none')(y_true, y_pred)

  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)
  loss = tf.multiply(loss, mask)

  return tf.reduce_mean(loss)

class CustomSchedule1(tf.keras.optimizers.schedules.LearningRateSchedule):

  def __init__(self, d_model, warmup_steps=4000):
    super(CustomSchedule1, self).__init__()
    
    self.d_model = tf.constant(d_model,dtype=tf.float32)
    self.warmup_steps = warmup_steps
    
  def get_config(self):
        return {"d_model": self.d_model,"warmup_steps":self.warmup_steps}
    
  def __call__(self, step):
    arg1 = tf.math.rsqrt(step)
    arg2 = step * (self.warmup_steps**-1.5)

    return tf.math.multiply(tf.math.rsqrt(self.d_model), tf.math.minimum(arg1, arg2))

sample_learning_rate1 = CustomSchedule1(d_model=128)

# plt.plot(sample_learning_rate1(tf.range(200000, dtype=tf.float32)))
# plt.ylabel("Learning Rate")
# plt.xlabel("Train Step")

# VOCAB_SIZE1=8054
# MAX_LENGTH =125
# clear backend
tf.keras.backend.clear_session()

learning_rate = CustomSchedule1(D_MODEL)

optimizer = tf.keras.optimizers.Adam(
    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)

def accuracy(y_true, y_pred):
  # ensure labels have shape (batch_size, MAX_LENGTH - 1)
  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH1 - 1))
  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)

# initialize and compile model within strategy scope
with strategy.scope():
  modelcc = transformer1(
      vocab_size=8199,
      num_layers=NUM_LAYERS,
      units=UNITS,
      d_model=D_MODEL,
      num_heads=NUM_HEADS,
      dropout=DROPOUT)

  modelcc.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])

modelcc.summary()

modelcc.load_weights(model_cc)
print("Loaded model from disk")

MAX_LENGTH1=125
def evaluate1(sentence):
  sentence = preprocess_sentence(sentence)

  sentence = tf.expand_dims(
      START_TOKEN1 + tokenizer1.encode(sentence) + END_TOKEN1, axis=0)

  output = tf.expand_dims(START_TOKEN1, 0)

  for i in range(MAX_LENGTH1):
    predictions = modelcc(inputs=[sentence, output], training=False)

    # select the last word from the seq_len dimension
    predictions = predictions[:, -1:, :]
    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)

    # return the result if the predicted_id is equal to the end token
    if tf.equal(predicted_id, END_TOKEN1[0]):
      break

    # concatenated the predicted_id to the output which is given to the decoder
    # as its input.
    output = tf.concat([output, predicted_id], axis=-1)

  return tf.squeeze(output, axis=0)


def predict1(sentence):
  prediction = evaluate1(sentence)

  predicted_sentence = tokenizer1.decode(
      [i for i in prediction if i < tokenizer1.vocab_size])

  # print('Input: {}'.format(sentence))
  # print("\n")
  # print('Output: {}'.format(predicted_sentence))

  return predicted_sentence

#prediction
question1 = data_train['questionText'][10]
answer1 = data_train['answerText'][10]
# print("\n")
# print("Questions:",question1)
# print("\n")
# print("Answers:",answer1)
# print("\n")
# output = predict1(question1)

#prediction
question1 = data_train['questionText'][50]
answer1 = data_train['answerText'][50]
# print("\n")
# print("Questions:",question1)
# print("\n")
# print("Answers:",answer1)
# print("\n")
# output = predict1(question1)

question1 = data_train['questionText'][70]
answer1 = data_train['answerText'][70]
# print("\n")
# print("Questions:",question1)
# print("\n")
# print("Answers:",answer1)
# print("\n")
# output = predict1(question1)

question = data1['prompt'][10]
answer = data1['utterance'][10]
emotion = data1['context'][10]
output = predict(question)

# print(question)
output = predict1(question)
#Transfomer_XL_CC

# print(question)
# print("\n")
# print(answer)
# print("\n")
# print(emotion)
# print("\n")
# print(predict(question))
#Transformer_XL_ED

"""**Emotion model**"""

predictor1 = ktrain.load_predictor(Emotion_model)

imp="I never feel like myself. I can’t even think straight anymore. I start stuttering and I can’t remember anything. I always get nervous and usually talk myself down but recently end up fighting with, what feels like, someone else. I don’t know why I feel this way, but I hate it."
predictor1.predict(imp)

#'sad', 'afraid', 'impressed', 'grateful', 'excited', 'angry',
#       'prepared', 'disgusted', 'Mental_and_Personal_issues',
#       'Family conflict_Relationship_intimacy_issues',
#       'Councelling_issues'

question = data1['prompt'][110]
answer = data1['utterance'][110]
emotion = data1['context'][110]
# print(question)
# print(answer)
# print(emotion)
# predictor1.predict(question)

"""i used to scare for darkness

i virtually thought so.. and i used to get sweatings

afraid

I am so upset and mad. I cant believe what has happened!

I never thought things would go like this.

angry

I was so happy when I moved in to my new home. My girlfriend and I have been saving for awhile and we finally got the keys last month. Now we can finally move on with our life plans.

What kind of house are you building? 

joyful


"""

# def fun():
#   while True:
#     imp4=input()
#     if(imp4 != "Hi"):
#       c="Bye"
#       break
#     else:
#       c="Hi"
#     print(c)
#   return c

#  d=fun()
#  print(d)

# imp=input()
# print(imp)
# type(imp)
def get_response(imp):
  if(imp == "hi"):
    out_emotion1="Emotion:"+" "+"Hi,My name is Arnold."+"\n"
    output1 = "BOT:"+" "+"Nice to meet you. What can I do for you?"
  elif(predictor1.predict(imp) == "sad"):
    out_emotion1="Emotion:"+" "+predictor1.predict(imp)+"\n"
    output1="BOT:"+" "+predict(imp)
  elif(predictor1.predict(imp) == "afraid"):
    out_emotion1="Emotion:"+" "+predictor1.predict(imp)+"\n"
    output1="BOT:"+" "+predict(imp)
  elif(predictor1.predict(imp) == "impressed"):
    out_emotion1="Emotion:"+" "+predictor1.predict(imp)+"\n"
    output1="BOT:"+" "+predict(imp)
  elif(predictor1.predict(imp) == "grateful"):
    out_emotion1="Emotion:"+" "+predictor1.predict(imp)+"\n"
    output1="BOT:"+" "+predict(imp)
  elif(predictor1.predict(imp) == "excited"):
    out_emotion1="Emotion:"+" "+predictor1.predict(imp)+"\n"
    output1="BOT:"+" "+predict(imp)
  elif(predictor1.predict(imp) == "angry"):
    out_emotion1="Emotion:"+" "+predictor1.predict(imp)+"\n"
    output1="BOT:"+" "+predict(imp)
  elif(predictor1.predict(imp) == "prepared"):
    out_emotion1="Emotion:"+" "+predictor1.predict(imp)+"\n"
    output1= "BOT:"+" "+predict(imp)
  elif(predictor1.predict(imp) == "disgusted"):
    out_emotion1="Emotion:"+" "+predictor1.predict(imp)+"\n"
    output1="BOT:"+" "+predict(imp)
  else:
    out_emotion1="Emotion:"+" "+predictor1.predict(imp)+"\n"
    output1="Dr. Arnold:" + predict1(imp)
   
  return out_emotion1,output1
  

if __name__ == "__main__":
    print("Let's chat! (type 'quit' to exit)")
    while True:
        # sentence = "do you use credit cards?"
        sentence = input("You: ")
        if sentence == "quit":
            break

        resp_emo,resp, = get_response(sentence)
        print(resp_emo)
        print(resp)

# def chatbot():
#   while True:
#     imp=print("User",input())
#     if(imp !="bye"):
#       if(imp == "hi"):
#         out_emotion1="\033[1mBOT:\033[0m Hi,My name is Arnold."
#         output1 = "Nice to meet you. What can I do for you?"
#       elif(predictor1.predict(imp) == "sad"):
#         out_emotion1=predictor1.predict(imp)
#         output1="\033[1mBOT:\033[0m"+ predict(imp)
#       elif(predictor1.predict(imp) == "afraid"):
#         out_emotion1=predictor1.predict(imp)
#         output1="\033[1mBOT:\033[0m"+ predict(imp)
#       elif(predictor1.predict(imp) == "impressed"):
#         out_emotion1=predictor1.predict(imp)
#         output1="\033[1mBOT:\033[0m"+ predict(imp)
#       elif(predictor1.predict(imp) == "grateful"):
#         out_emotion1=predictor1.predict(imp)
#         output1="\033[1mBOT:\033[0m"+ predict(imp)
#       elif(predictor1.predict(imp) == "excited"):
#         out_emotion1=predictor1.predict(imp)
#         output1="\033[1mBOT:\033[0m"+ predict(imp)
#       elif(predictor1.predict(imp) == "angry"):
#         out_emotion1=predictor1.predict(imp)
#         output1="\033[1mBOT:\033[0m"+ predict(imp)
#       elif(predictor1.predict(imp) == "prepared"):
#         out_emotion1=predictor1.predict(imp)
#         output1="\033[1mBOT:\033[0m"+ predict(imp)
#       elif(predictor1.predict(imp) == "disgusted"):
#         out_emotion1=predictor1.predict(imp)
#         output1="\033[1mBOT:\033[0m"+ predict(imp)
#       else:
#         out_emotion1=predictor1.predict(imp)
#         output1=f"\033[1mDr. BOT: {predict1(imp)} \033[0m"
#     else:
#         print("Bye")
#         break
#     print(out_emotion1)
#     print(output1)
#   return out_emotion1,output1

# emotion2,output2=chatbot()

# print(emotion)
# print(output)

print(data1['prompt'][110])
print(data1['prompt'][10])
print(data1['prompt'][20])
print(data1['prompt'][30])
print(data1['prompt'][40])

print(data_train['questionText'][101])
print(data_train['questionText'][200])
print(data_train['questionText'][300])
print(data_train['questionText'][400])
print(data_train['questionText'][500])

#'sad', 'afraid', 'impressed', 'grateful', 'excited', 'angry',
#       'prepared', 'disgusted', 'Mental_and_Personal_issues',
#       'Family conflict_Relationship_intimacy_issues',
#       'Councelling_issues'

# from Emchatbot import chatbot

# from EMHchatbot import chatbot

# chatbot()

